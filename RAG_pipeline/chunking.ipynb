{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a75e315",
   "metadata": {},
   "source": [
    "## Chunking Strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e8b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import contractions\n",
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7afd79",
   "metadata": {},
   "source": [
    "* **Function: `chunk_text_file`**\n",
    "\n",
    "  * Initializes a `TokenTextSplitter` from LangChain with:\n",
    "\n",
    "    * `encoding_name`: which tokenizer to use (`cl100k_base`).\n",
    "    * `chunk_size`: how many tokens per chunk (default 512).\n",
    "    * `chunk_overlap`: how many tokens overlap between chunks (default 50).\n",
    "  * Splits the text into token-based chunks using `text_splitter.split_text`.\n",
    "  * Prints:\n",
    "\n",
    "    * Approximate word count of the original text.\n",
    "    * Number of chunks generated.\n",
    "  * Returns the list of chunks.\n",
    "\n",
    "* **Function: `save_chunks_to_json`**\n",
    "\n",
    "  * Iterates over the chunks list.\n",
    "  * For each chunk, creates a dictionary with:\n",
    "\n",
    "    * `chunk_id`: sequential ID (starting from 1).\n",
    "    * `text`: the chunked text content.\n",
    "    * `token_count`: rough count of tokens (using `.split()` on whitespace, not exact).\n",
    "  * Collects all chunk dictionaries into a list under `\"chunks\"`.\n",
    "  * Saves the data as a formatted JSON file (`output_file`).\n",
    "  * Prints confirmation of where the chunks were saved.\n",
    "\n",
    "* **Main Execution (`if __name__ == \"__main__\":`)**\n",
    "\n",
    "  * Defines the input file path (`cleaned_text_content.txt`) and output file path (`chunks.json`).\n",
    "  * Calls `chunk_text_file` to generate token-based chunks.\n",
    "  * Calls `save_chunks_to_json` to store the chunks into a JSON file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9709364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length: 84155 words (approx)\n",
      "Number of chunks created: 227\n",
      "\n",
      "Chunks saved to ./data/chunks.json\n"
     ]
    }
   ],
   "source": [
    "def chunk_text_file(file_path, chunk_size=512, chunk_overlap=50, encoding_name=\"cl100k_base\"):\n",
    "    \"\"\"\n",
    "    Chunk a text file into fixed-size chunks based on tokens using LangChain\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file\n",
    "        chunk_size (int): Number of tokens per chunk\n",
    "        chunk_overlap (int): Number of tokens to overlap between chunks\n",
    "        encoding_name (str): Tokenizer encoding (cl100k_base works with OpenAI models)\n",
    "\n",
    "    Returns:\n",
    "        list: List of text chunks\n",
    "    \"\"\"\n",
    "    # Read the file content\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Initialize LangChain TokenTextSplitter\n",
    "    text_splitter = TokenTextSplitter(\n",
    "        encoding_name=encoding_name,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    # Split text into chunks\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    print(f\"Original length: {len(text.split())} words (approx)\")\n",
    "    print(f\"Number of chunks created: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_chunks_to_json(chunks, output_file=\"chunks.json\"):\n",
    "    \"\"\"\n",
    "    Save chunks to a JSON file with metadata\n",
    "    \"\"\"\n",
    "    chunks_data = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_info = {\n",
    "            \"chunk_id\": i + 1,\n",
    "            \"text\": chunk,\n",
    "            \"token_count\": len(chunk.split())  # rough token count, not exact\n",
    "        }\n",
    "        chunks_data.append(chunk_info)\n",
    "\n",
    "    output_data = {\"chunks\": chunks_data}\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nChunks saved to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"../Scraping_Digico_Website/scraped_data/cleaned_text_content.txt\"\n",
    "    output_json = \"./data/chunks.json\"\n",
    "\n",
    "    # Fixed-size token chunking\n",
    "    chunks = chunk_text_file(file_path, chunk_size=512, chunk_overlap=50)\n",
    "    save_chunks_to_json(chunks, output_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862550b2",
   "metadata": {},
   "source": [
    "## Data enriching section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d164b139",
   "metadata": {},
   "source": [
    "In NLP, removing stopwords (common words like *the, is, and, of*) helps reduce noise and focus on the more meaningful terms in a text. Since stopwords occur very frequently but carry little semantic value, eliminating them can make text processing more efficient, reduce the size of the data, and improve the performance of models that rely on distinguishing informative words for tasks like semantic search\n",
    "\n",
    "Note : there is stop words such as \"not\" that can change the meaning of a sentence, and that's why I defined a list of stopwords that should not be removed because they play a crucial role in building the meaning\n",
    "\n",
    "Another note : I know that this list does not contain all the words that hold meaning, but this is still better than removing all stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b2dda1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load standard English stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Keep negation-related words\n",
    "negation_words = {\"not\", \"no\", \"nor\", \"never\", \"very\", \"too\", \"just\", \"only\", \"even\", \"almost\", \"but\", \"however\", \"although\", \"though\", \"yet\", \"before\", \"after\",  \"until\",  \"since\",  \"because\",  \"if\",  \"unless\",  \"while\", \"all\",  \"any\",  \"few\", \"most\", \"some\", \"many\", \"several\", \"more\", \"less\", \"can\", \"could\", \"should\", \"would\", \"may\", \"might\", \"must\", \"shall\", \"not\", \"no\", \"nor\", \"never\", \"none\", \"nothing\", \"neither\", \"nowhere\", \"hardly\", \"scarcely\", \"barely\"}\n",
    "stop_words = stop_words - negation_words\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered = [w for w in words if w not in stop_words]\n",
    "    return \" \".join(filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdbb2f4",
   "metadata": {},
   "source": [
    "Ponctuation induces noise and does not help with building the meaning so we need to remove it (While writing the documentation I noticed that I could have used a library for catching punctuation)\n",
    "\n",
    "in addition output of TokenTextSplitter contains strings that make noise : \n",
    "example : \n",
    "'\\n\\n'\n",
    "'\\n'\n",
    "'\\'\n",
    "\n",
    "Also :\n",
    "- I used contractions tool to expand contractions first (isn't -> is not, don't -> do not, etc.)\n",
    "- I applied lowercasing (because the embedding vector of Laptop is different than the embedding vector of laptop and this may make some confusions)\n",
    "- many lines start with \". \" so I removed it \n",
    "- I normalized the spaces\n",
    "- I thought of performing lemmatization but for the baseline version I choose not to overcomplicate my approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6825143d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is saved to data\\chunks_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Input/output paths\n",
    "input_path = Path(\"./data/chunks.json\")\n",
    "output_path = Path(\"./data/chunks_cleaned.json\")\n",
    "\n",
    "# Replacement rules\n",
    "replacements = [\n",
    "    ('\\n\\n', ' '),\n",
    "    ('\\n', ' '),\n",
    "    ('\\\"', ''),\n",
    "    ('. ', ' '),\n",
    "    (', ', ' '),\n",
    "    ('; ', ' '),\n",
    "    (': ', ' ')\n",
    "]\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned = text\n",
    "\n",
    "    # Expand contractions first (isn't -> is not, don't -> do not, etc.)\n",
    "    cleaned = contractions.fix(cleaned)\n",
    "    \n",
    "    # Apply simple replacements\n",
    "    for old, new in replacements:\n",
    "        cleaned = cleaned.replace(old, new)\n",
    "\n",
    "    # If line starts with \". \" remove it\n",
    "    cleaned = re.sub(r'^\\ \\s+', '', cleaned)\n",
    "\n",
    "    # Normalize spaces \n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    # lowercase the cleaned text\n",
    "    cleaned = cleaned.lower()\n",
    "\n",
    "    cleaned = remove_stopwords(cleaned)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "def main():\n",
    "    # Load original file\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    cleaned_chunks = {\"chunks\": []}\n",
    "\n",
    "    for chunk in data.get(\"chunks\", []):\n",
    "        original_text = chunk.get(\"text\", \"\")\n",
    "        cleaned_text = clean_text(original_text)\n",
    "\n",
    "        cleaned_chunks[\"chunks\"].append({\n",
    "            \"chunk_id\": chunk.get(\"chunk_id\"),\n",
    "            \"original_text\": original_text,\n",
    "            \"cleaned_text\": cleaned_text\n",
    "        })\n",
    "\n",
    "    # Save new file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cleaned_chunks, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"The output is saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
